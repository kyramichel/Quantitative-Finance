{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "This notbook illustrates the steps in a **Fairness Model Review** process applied to a **credit risk model**. The review ensures compliance with fairness standards and prevents discriminatory practices in decision-making:\n",
        "\n",
        "1. Identify and remove **prohibited variables** (such as **race**, **gender**, **age**) from the model to prevent biased decision-making.\n",
        "\n",
        "2. Analyze if any variables act as **proxy variables** that might indirectly reflect prohibited characteristics (e.g., ZIP code could correlate with race).\n",
        "\n",
        "3. Document & Justify Variables used in the model to ensure they are non-discriminatory and comply with fairness standards.\n",
        "\n",
        "4. Evaluate the model for potential biases in predictions. If disparities are identified, apply mitigation strategies or alternative approaches to balance fairness across different groups.\n",
        "\n",
        "This review process promotes **fairness** in decision-making.\n",
        "\n"
      ],
      "metadata": {
        "id": "4FaBFGRtGTL8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score"
      ],
      "metadata": {
        "id": "PEAXH3cmJBGc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Create a dataset with some potentially sensitive variables (e.g., race, gender)\n",
        "data = {\n",
        "    'income': [45000, 54000, 60000, 67000, 75000],\n",
        "    'credit_score': [620, 650, 700, 750, 800],\n",
        "    'age': [25, 40, 35, 50, 60],\n",
        "    'loan_amount': [20000, 25000, 30000, 35000, 40000],\n",
        "    'gender': ['male', 'female', 'female', 'male', 'female'],  # Prohibited attribute\n",
        "    'race': ['Caucasian', 'African American', 'Caucasian', 'Hispanic', 'Caucasian'],  # Prohibited attribute\n",
        "    'approved': [1, 0, 1, 1, 0]  # Target variable\n",
        "}\n",
        "\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# Step 1: Identify & Remove prohibited variables (race and gender)\n",
        "df_clean = df.drop(columns=['gender', 'race'])\n",
        "\n",
        "print(\"Cleaned dataset (after removing prohibited variables):\")\n",
        "print(df_clean)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "db6HuSg5PMzL",
        "outputId": "0710f3b5-c001-4390-949a-dbb00325f75b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cleaned dataset (after removing prohibited variables):\n",
            "   income  credit_score  age  loan_amount  approved\n",
            "0   45000           620   25        20000         1\n",
            "1   54000           650   40        25000         0\n",
            "2   60000           700   35        30000         1\n",
            "3   67000           750   50        35000         1\n",
            "4   75000           800   60        40000         0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 2: Analyze if any variables might act as proxy variables for sensitive attributes like race. For example, income may correlate with race or zip code, potentially introducing indirect bias."
      ],
      "metadata": {
        "id": "DtfUnsCpPS_2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# check for potential proxy variables (eg income and age correlation)\n",
        "correlations = df[['income', 'credit_score', 'age', 'loan_amount']].corr()\n",
        "\n",
        "print(\"\\nCorrelation matrix to identify proxy variables:\")\n",
        "print(correlations)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ihth5M8mPU_k",
        "outputId": "54c00c45-3bad-4f18-e3de-8bccd4e1b1af"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Correlation matrix to identify proxy variables:\n",
            "                income  credit_score       age  loan_amount\n",
            "income        1.000000      0.990916  0.949069     0.998222\n",
            "credit_score  0.990916      1.000000  0.927740     0.996241\n",
            "age           0.949069      0.927740  1.000000     0.936329\n",
            "loan_amount   0.998222      0.996241  0.936329     1.000000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can see, they are correlate highly, so we’d need to investigate further to see if income could act as a proxy for age.\n"
      ],
      "metadata": {
        "id": "DIeJp36oPc2X"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Step 3: Document and justify each variable used in the model to ensure non-discrimination"
      ],
      "metadata": {
        "id": "hh3Ie2poPyoa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "variable_documentation = {\n",
        "    'income': 'Represents the borrower’s ability to repay the loan. It is not a proxy for race or gender.',\n",
        "    'credit_score': 'A commonly used metric in assessing credit risk, not discriminatory.',\n",
        "    'age': 'Age is a neutral factor that is not prohibited under Fair Lending laws.',\n",
        "    'loan_amount': 'The loan amount requested is a non-discriminatory factor for loan approval.',\n",
        "}\n",
        "\n",
        "print(\"\\nVariable documentation and justification:\")\n",
        "for var, justification in variable_documentation.items():\n",
        "    print(f\"{var}: {justification}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q4f8e3dWP80h",
        "outputId": "d641ee93-dcdb-489f-c1c3-9ab6bd66ec1c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Variable documentation and justification:\n",
            "income: Represents the borrower’s ability to repay the loan. It is not a proxy for race or gender.\n",
            "credit_score: A commonly used metric in assessing credit risk, not discriminatory.\n",
            "age: Age is a neutral factor that is not prohibited under Fair Lending laws.\n",
            "loan_amount: The loan amount requested is a non-discriminatory factor for loan approval.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 4: Check for Disparate Impact (DI) in the predictions\n",
        "\n",
        "If a certain group (e.g., Caucasian vs African American) experiences different outcomes, we apply fairness mitigation."
      ],
      "metadata": {
        "id": "Y0FlvDEdQBKL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Prepare features and target\n",
        "X = df_clean.drop(columns=['approved'])\n",
        "y = df_clean['approved']\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train a simple logistic regression model\n",
        "model = LogisticRegression()\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Evaluate the model\n",
        "y_pred = model.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "\n",
        "# Step 4: Disparate Impact analysis (for simplicity, let's assume race was a feature)\n",
        "# Normally, we'd check if any group is unfairly disadvantaged\n",
        "# For now, we'll print a simple statement (in practice, use fairness metrics like statistical parity)\n",
        "\n",
        "if accuracy < 0.75:  # Placeholder condition to apply fairness mitigation\n",
        "    print(\"Disparate Impact detected, applying fairness mitigation...\")\n",
        "    # Placeholder for fairness remediation (e.g., apply re-weighting or re-sampling)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hgGKmhIeQX3m",
        "outputId": "f1b57fbe-1886-4e23-b420-013e03aff84e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Disparate Impact detected, applying fairness mitigation...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note: In this simple example, we evaluated the model's fairness using a simple accuracy threshold (which in practice would be replaced by fairness metrics like DI).\n",
        "\n",
        "-If significant disparities are found in disparate impact analysis, then:\n",
        "\n",
        "Step 5: Implement mitigation techniques (such as re-weighting, re-sampling, or modifying the model) to improve fairness (alternative analysis)"
      ],
      "metadata": {
        "id": "yKWPA3dyQqHR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Key metrics:\n",
        "\n",
        "**Disparate Impact (DI)**:\n",
        "\n",
        "$\\text{Disparate Impact} = \\frac{\\text{Approval Rate for Minority Group}}{\\text{Approval Rate for Majority Group}} $\n",
        "\n",
        "If DI < 0.8, it may indicate disparate impact, suggesting that the model might be biased against certain groups.\n",
        "\n",
        " **Fairness metrics** that help assess potential biases or disparities in a model's decision-making process, particularly in terms of different groups or protected classes (e.g., race, gender):\n",
        "\n",
        "**AIR** is used to evaluate the fairness of a decision-making process, particularly in terms of DI between 2 groups:\n",
        "\n",
        "$\n",
        "\\text{AIR} = \\frac{\\text{Selection Rate for Protected Group}}{\\text{Selection Rate for Reference Group}}$\n",
        "\n",
        "where *Selection Rate* = proportion of applicants in a group who receive the favorable decision (e.g., loan approval, promotion)\n",
        "\n",
        "- **AIR = 1**: Perfect equality between the protected and reference groups.\n",
        "- **AIR < 0.8** is often considered an indication of **adverse impact**, where the protected group is unfairly disadvantaged in terms of the favorable decision.\n",
        "- **AIR > 1**: Indicates that the protected group is favored over the reference group\n",
        "\n",
        "Example:If the selection rate for a protected group (e.g., women) is 40% and for the reference group (e.g., men) is 60%, then:\n",
        "$\\text{AIR} = \\frac{0.40}{0.60} = 0.67$.\n",
        "An AIR below 0.8 would indicate adverse impact against women in this case.\n",
        "\n",
        "\n",
        "**Standardized Mean Difference (SMD)** measures the **difference in means** between 2 groups in a standardized way to assess the degree of disparity or imbalance between 2 groups on a particular feature or outcome:\n",
        "\n",
        "($\\mu_1 - \\mu_2)/\\sigma$\n",
        "\n",
        "- **SMD = 0**: No difference between the two groups.\n",
        "- **SMD between 0 and 0.2**: Small difference between groups.\n",
        "- **SMD between 0.2 and 0.5**: Moderate difference between groups.\n",
        "- **SMD > 0.5**: Large difference between groups, suggesting significant imbalance or disparity in the variable being assessed.\n",
        "\n",
        "\n",
        "Summary:\n",
        "- **AIR** helps evaluate whether one group is disproportionately affected by adverse decisions.\n",
        "- **SMD** provides insights into whether there is a significant imbalance or disparity in the distribution of a key feature between different groups.\n",
        "\n",
        "Together, these 2 fairness metrics help identify where fairness interventions might be necessary\n"
      ],
      "metadata": {
        "id": "vvLLmYV1R2Hz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Conclusion:**\n",
        "\n",
        "For more complex or production-level datasets, it's highly recommended to use fairness auditing toolkits like **AI Fairness 360 (AIF360)** - an open-source library developed by IBM to help developers and data scientists evaluate and improve the fairness of their AI systems.\n",
        "\n",
        "\n",
        "**AIF360** and **Fairlearn** libraries offer a wide range of metrics (e.g., **Disparate Impact**, **Equalized Odds**, **Demographic Parity**) and mitigation techniques (like **reweighing**, **disparate impact remover**, and **adversarial debiasing**) that help assess and reduce unfair bias in machine learning models. They are especially useful when working with **non-linear models** like **Gradient Boosted Machines (GBM)**, where bias can be subtle and embedded in feature interactions.\n",
        "\n",
        "For example: *!pip install aif360[Reductions]* - to install AI Fairness 360 with the additional fairness algorithms related to reductions, that can be used for detecting & mitigating bias in ML models.\n"
      ],
      "metadata": {
        "id": "fg9xHwFfY7iS"
      }
    }
  ]
}